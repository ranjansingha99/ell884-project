{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11187115,"sourceType":"datasetVersion","datasetId":6983578}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Install Dependencies","metadata":{}},{"cell_type":"code","source":"# Install dependencies\n!pip install transformers datasets torch pandas numpy nltk sacrebleu rouge-score bert-score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom rouge_score import rouge_scorer\nimport bert_score\nimport warnings\nfrom transformers import TrainerCallback\nimport logging\nimport os\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"Setup complete!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load DialoCONAN Dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport os\n\n# Define possible dataset paths and file names\nbase_paths = [\n    \"/kaggle/input/dialoconan\",\n    \"/kaggle/input/dialoconan-dataset\",\n    \"/kaggle/input/DIALOCONAN\",\n    \"/kaggle/input\"\n]\nfile_names = [\n    \"DIALOCONAN.csv\",\n    \"dialoconan.csv\",\n    \"DIALOCONAN.CSV\",\n    \"dialoconan.CSV\"\n]\n\n# Try loading the dataset\ndf = None\nfor base_path in base_paths:\n    for file_name in file_names:\n        dataset_path = os.path.join(base_path, file_name)\n        if os.path.exists(dataset_path):\n            try:\n                df = pd.read_csv(dataset_path)\n                print(f\"Successfully loaded dataset from: {dataset_path}\")\n                break\n            except Exception as e:\n                print(f\"Error reading {dataset_path}: {e}\")\n    if df is not None:\n        break\n\n# If dataset not found, list available files for debugging\nif df is None:\n    print(\"Dataset not found at any specified paths. Available files in /kaggle/input:\")\n    for dirname, _, filenames in os.walk('/kaggle/input'):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))\n    raise FileNotFoundError(\n        \"Could not find DIALOCONAN.csv. Please verify the dataset is uploaded to /kaggle/input/dialoconan/ \"\n        \"or another directory, and check the file name for case sensitivity.\"\n    )\n\n# Display basic info\nprint(\"Dataset Shape:\", df.shape)\nprint(\"Columns:\", df.columns.tolist())\nprint(\"Sample Data:\")\nprint(df.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Preprocess the data","metadata":{}},{"cell_type":"code","source":"def create_dialogue_pairs(df):\n    dialogues = []\n    for dialogue_id in df[\"dialogue_id\"].unique():\n        dialogue = df[df[\"dialogue_id\"] == dialogue_id].sort_values(\"turn_id\").reset_index(drop=True)\n        context = []\n        for _, row in dialogue.iterrows():\n            if row[\"type\"] == \"HS\":\n                context.append(f\"[{row['TARGET']} HS]: {row['text']}\")\n            elif row[\"type\"] == \"CN\":\n                # Find previous turn using turn_id\n                prev_turn = dialogue[dialogue[\"turn_id\"] == row[\"turn_id\"] - 1]\n                hs_text = \"\"\n                if not prev_turn.empty and prev_turn.iloc[0][\"type\"] == \"HS\":\n                    hs_text = prev_turn.iloc[0][\"text\"]\n                else:\n                    logging.warning(f\"Dialogue {dialogue_id}, turn_id {row['turn_id']}: No valid preceding HS turn, skipping\")\n                    continue\n                \n                # Create input/output pair\n                input_text = \" \".join(context[-5:]) if context else \"\"\n                dialogues.append({\n                    \"input\": input_text,\n                    \"output\": row[\"text\"],\n                    \"target\": row[\"TARGET\"],\n                    \"hs_text\": hs_text,\n                    \"cn_text\": row[\"text\"]\n                })\n                context.append(f\"[{row['TARGET']} CN]: {row['text']}\")\n    return dialogues\n\n# Create dialogue pairs\ndata = create_dialogue_pairs(df)\nprint(f\"Total Dialogue Pairs: {len(data)}\")\n\n# Split data\ntrain_data, temp_data = train_test_split(data, test_size=0.2, random_state=42)\nval_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n\n# Convert to Hugging Face Dataset\ntrain_dataset = Dataset.from_list(train_data)\nval_dataset = Dataset.from_list(val_data)\ntest_dataset = Dataset.from_list(test_data)\n\nprint(f\"Train Size: {len(train_dataset)}, Val Size: {len(val_dataset)}, Test Size: {len(test_dataset)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Tokenize Dataset","metadata":{}},{"cell_type":"code","source":"# Load tokenizer\nmodel_name = \"t5-large\"\ntokenizer = T5Tokenizer.from_pretrained(model_name)\n\n# Tokenize function\ndef tokenize(batch):\n    inputs = tokenizer(\n        batch[\"input\"],\n        max_length=512,\n        truncation=True,\n        padding=\"max_length\",\n        return_tensors=\"np\"\n    )\n    outputs = tokenizer(\n        batch[\"output\"],\n        max_length=128,\n        truncation=True,\n        padding=\"max_length\",\n        return_tensors=\"np\"\n    )\n    return {\n        \"input_ids\": inputs.input_ids,\n        \"attention_mask\": inputs.attention_mask,\n        \"labels\": outputs.input_ids,\n        \"input\": batch[\"input\"],\n        \"output\": batch[\"output\"],\n        \"target\": batch[\"target\"],\n        \"hs_text\": batch[\"hs_text\"],\n        \"cn_text\": batch[\"cn_text\"]\n    }\n\n# Apply tokenization\ntrain_dataset = train_dataset.map(tokenize, batched=True)\nval_dataset = val_dataset.map(tokenize, batched=True)\ntest_dataset = test_dataset.map(tokenize, batched=True)\n\n# Set format for training\ntrain_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\", \"input\", \"output\", \"target\", \"hs_text\", \"cn_text\"])\nval_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\", \"input\", \"output\", \"target\", \"hs_text\", \"cn_text\"])\ntest_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\", \"input\", \"output\", \"target\", \"hs_text\", \"cn_text\"])\n\nprint(\"Tokenization complete!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Initialize Model","metadata":{}},{"cell_type":"code","source":"# Load model\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\n\n# Move to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nprint(f\"Model loaded on {device}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Fine-Tune T5 Model","metadata":{}},{"cell_type":"code","source":"# Suppress warnings\nwarnings.filterwarnings(\"ignore\", message=\"Some weights of\")\n\n# Set up logging\nlogging.basicConfig(\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    level=logging.INFO,\n    handlers=[\n        logging.StreamHandler(),\n        logging.FileHandler(\"/kaggle/working/training_logs.txt\")\n    ]\n)\nlogger = logging.getLogger(__name__)\n\n# Custom callback for validation metrics\nclass CustomMetricsCallback(TrainerCallback):\n    def __init__(self, tokenizer, val_dataset, max_samples=100):\n        self.tokenizer = tokenizer\n        self.val_dataset = val_dataset.select(range(min(max_samples, len(val_dataset))))\n        self.scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n\n    def on_evaluate(self, args, state, control, model, **kwargs):\n        logger.info(f\"Computing custom metrics at step {state.global_step}\")\n        bleu_scores, rouge_scores, bert_scores = [], [], []\n        \n        model.eval()\n        for item in self.val_dataset:\n            try:\n                input_text = item[\"input\"]\n                target = item[\"target\"]\n                \n                # Generate prediction\n                inputs = self.tokenizer(\n                    input_text,\n                    return_tensors=\"pt\",\n                    max_length=512,\n                    truncation=True\n                ).to(model.device)\n                outputs = model.generate(\n                    inputs[\"input_ids\"],\n                    max_length=128,\n                    num_beams=5,\n                    no_repeat_ngram_size=2,\n                    early_stopping=True\n                )\n                pred = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n                ref = item[\"cn_text\"]\n                \n                # Compute metrics\n                bleu_scores.append(sentence_bleu([ref.split()], pred.split()))\n                rouge_scores.append(self.scorer.score(ref, pred)[\"rougeL\"].fmeasure)\n                P, R, F1 = bert_score.score([pred], [ref], lang=\"en\", rescale_with_baseline=True)\n                bert_scores.append(F1.item())\n            except Exception as e:\n                logger.warning(f\"Error processing item: {e}\")\n                continue\n        \n        # Log metrics\n        metrics = {\n            \"val_bleu\": np.mean(bleu_scores) if bleu_scores else 0.0,\n            \"val_rouge_l\": np.mean(rouge_scores) if rouge_scores else 0.0,\n            \"val_bertscore\": np.mean(bert_scores) if bert_scores else 0.0\n        }\n        logger.info(f\"Validation Metrics: {metrics}\")\n        return None\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/t5-counterspeech\",\n    num_train_epochs=3,  # Reduced to avoid runtime limit\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=4,  # Added to stabilize training\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir=\"/kaggle/working/logs\",\n    logging_steps=100,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    save_total_limit=1,\n    fp16=True,\n    report_to=\"none\",\n)\n\n# Initialize trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    callbacks=[CustomMetricsCallback(tokenizer, val_dataset)]\n)\n\n# Train model\nlogger.info(\"Starting training...\")\ntrainer.train()\n\n# Save model\nmodel.save_pretrained(\"/kaggle/working/t5-counterspeech-final\")\ntokenizer.save_pretrained(\"/kaggle/working/t5-counterspeech-final\")\n\n# Save logs\nwith open(\"/kaggle/working/final_training_logs.txt\", \"w\") as f:\n    f.write(str(trainer.state.log_history))\n\nprint(\"Training complete! Check /kaggle/working/training_logs.txt for detailed logs.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Plot training/validation loss","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nlogs = trainer.state.log_history\nsteps = [log[\"step\"] for log in logs if \"loss\" in log]\nlosses = [log[\"loss\"] for log in logs if \"loss\" in log]\nplt.plot(steps, losses, label=\"Training Loss\")\nplt.xlabel(\"Batch Steps\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generate Counterspeech on test-split","metadata":{}},{"cell_type":"code","source":"def generate_counterspeech(hs_text, target, dialogue_history=None):\n    # Prepare input\n    input_text = f\"[{target} HS]: {hs_text}\" if not dialogue_history else f\"{dialogue_history} [{target} HS]: {hs_text}\"\n    \n    # Tokenize\n    inputs = tokenizer(\n        input_text,\n        return_tensors=\"pt\",\n        max_length=512,\n        truncation=True\n    ).to(device)\n    \n    # Generate\n    outputs = model.generate(\n        inputs[\"input_ids\"],\n        max_length=128,\n        num_beams=5,\n        no_repeat_ngram_size=2,\n        early_stopping=True\n    )\n    \n    # Decode\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return response\n\n# Test inference\nsample_hs = \"All migrants are criminals and should be deported!\"\nsample_target = \"MIGRANTS\"\nsample_history = \"[MIGRANTS HS]: More migrants crossing the channel today. We donâ€™t have enough accommodation. [MIGRANTS CN]: Are you forgetting that last year every rough sleeper was offered a bed during lockdown?\"\ngenerated_cn = generate_counterspeech(sample_hs, sample_target, sample_history)\nprint(f\"Hate Speech: {sample_hs}\")\nprint(f\"Generated Counterspeech: {generated_cn}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluate Model","metadata":{}},{"cell_type":"code","source":"def evaluate_model(dataset):\n    bleu_scores = []\n    rouge_scores = []\n    bert_scores = []\n    scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n    \n    for item in dataset:\n        # Generate prediction\n        pred = generate_counterspeech(item[\"hs_text\"], item[\"target\"], item[\"input\"])\n        ref = item[\"cn_text\"]\n        \n        # BLEU\n        bleu_scores.append(sentence_bleu([ref.split()], pred.split()))\n        \n        # ROUGE\n        rouge_scores.append(scorer.score(ref, pred)[\"rougeL\"].fmeasure)\n        \n        # BERTScore\n        P, R, F1 = bert_score.score([pred], [ref], lang=\"en\", rescale_with_baseline=True)\n        bert_scores.append(F1.item())\n    \n    return {\n        \"BLEU\": np.mean(bleu_scores),\n        \"ROUGE-L\": np.mean(rouge_scores),\n        \"BERTScore\": np.mean(bert_scores)\n    }\n\n# Run evaluation\nresults = evaluate_model(test_dataset)\nprint(\"Evaluation Results:\")\nfor metric, score in results.items():\n    print(f\"{metric}: {score:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Save Results","metadata":{}},{"cell_type":"code","source":"# Save evaluation results\nimport json\nwith open(\"/kaggle/working/evaluation_results.json\", \"w\") as f:\n    json.dump(results, f, indent=4)\n\n# Save sample predictions\npredictions = []\nfor item in test_dataset.select(range(5)):\n    pred = generate_counterspeech(item[\"hs_text\"], item[\"target\"], item[\"input\"])\n    predictions.append({\n        \"input\": item[\"input\"],\n        \"hate_speech\": item[\"hs_text\"],\n        \"target\": item[\"target\"],\n        \"predicted_counterspeech\": pred,\n        \"reference_counterspeech\": item[\"cn_text\"]\n    })\n\npd.DataFrame(predictions).to_csv(\"/kaggle/working/sample_predictions.csv\", index=False)\nprint(\"Results and predictions saved!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}